{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medical Diagnosis LLM via Low-Rank Adaptation and Retrieval-Augmentation\n",
    "\n",
    "A complete RAG pipeline for medical question answering using:\n",
    "- **PubMedQA** dataset (211K+ biomedical QA pairs)\n",
    "- **PubMedBERT** for domain-specific embeddings\n",
    "- **ChromaDB** vector database for evidence retrieval\n",
    "- **BioMistral-7B + LoRA** for fine-tuned medical reasoning\n",
    "- **Safety guardrails** for responsible AI in healthcare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), '..', 'src'))\n",
    "os.chdir(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "from data_loader import load_config, load_pubmedqa, create_train_val_test_split, format_for_training\n",
    "\n",
    "config = load_config()\n",
    "print('Configuration loaded successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Data Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PubMedQA datasets\n",
    "data = load_pubmedqa(config)\n",
    "\n",
    "labeled_df = data['labeled']\n",
    "artificial_df = data['artificial']\n",
    "\n",
    "print(f'\\n=== Dataset Summary ===')\n",
    "print(f'Labeled (expert-annotated): {len(labeled_df)} examples')\n",
    "print(f'Artificial (machine-generated): {len(artificial_df)} examples')\n",
    "print(f'\\nLabeled columns: {list(labeled_df.columns)}')\n",
    "print(f'\\nLabel distribution (labeled):')\n",
    "print(labeled_df['final_decision'].value_counts())\n",
    "print(f'\\nLabel distribution (artificial):')\n",
    "print(artificial_df['final_decision'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample examples\n",
    "print('=== Sample Examples ===')\n",
    "for i in range(3):\n",
    "    row = labeled_df.iloc[i]\n",
    "    print(f'\\n--- Example {i+1} ---')\n",
    "    print(f'Question: {row[\"question\"]}')\n",
    "    print(f'Context: {row[\"context\"][:200]}...')\n",
    "    print(f'Answer: {row[\"final_decision\"]}')\n",
    "    print(f'Explanation: {row[\"long_answer\"][:150]}...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize dataset\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Label distribution\n",
    "colors = ['#2ecc71', '#e74c3c', '#f39c12']\n",
    "labeled_df['final_decision'].value_counts().plot(kind='bar', ax=axes[0], color=colors)\n",
    "axes[0].set_title('Label Distribution (Labeled Set)')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].tick_params(axis='x', rotation=0)\n",
    "\n",
    "# Context length distribution\n",
    "labeled_df['context_len'] = labeled_df['context'].str.len()\n",
    "axes[1].hist(labeled_df['context_len'], bins=30, color='#3498db', alpha=0.7)\n",
    "axes[1].set_title('Context Length Distribution')\n",
    "axes[1].set_xlabel('Characters')\n",
    "axes[1].set_ylabel('Count')\n",
    "\n",
    "# Question length distribution\n",
    "labeled_df['question_len'] = labeled_df['question'].str.split().str.len()\n",
    "axes[2].hist(labeled_df['question_len'], bins=20, color='#9b59b6', alpha=0.7)\n",
    "axes[2].set_title('Question Length Distribution (words)')\n",
    "axes[2].set_xlabel('Words')\n",
    "axes[2].set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('fig1_data_overview.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Val/Test split\n",
    "train_df, val_df, test_df = create_train_val_test_split(labeled_df, config)\n",
    "\n",
    "# Show training format\n",
    "print('\\n=== Training Prompt Format ===')\n",
    "print(format_for_training(train_df.iloc[0])[:500])\n",
    "print('...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Embedding Generation with PubMedBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from embeddings import PubMedBERTEmbedder\n",
    "\n",
    "embedder = PubMedBERTEmbedder(config)\n",
    "print(f'Embedding dimension: {embedder.get_embedding_dim()}')\n",
    "\n",
    "# Test with medical texts\n",
    "test_texts = [\n",
    "    'Does aspirin reduce the risk of heart attack?',\n",
    "    'Aspirin has been shown to reduce platelet aggregation and lower cardiovascular risk.',\n",
    "    'The treatment of diabetes involves insulin management.',\n",
    "    'Python is a programming language used for data science.',\n",
    "]\n",
    "\n",
    "test_embeddings = embedder.embed_texts(test_texts)\n",
    "print(f'\\nEmbedding shape: {test_embeddings.shape}')\n",
    "\n",
    "# Similarity matrix\n",
    "from numpy.linalg import norm\n",
    "sim_matrix = np.zeros((len(test_texts), len(test_texts)))\n",
    "for i in range(len(test_texts)):\n",
    "    for j in range(len(test_texts)):\n",
    "        sim_matrix[i][j] = np.dot(test_embeddings[i], test_embeddings[j]) / (\n",
    "            norm(test_embeddings[i]) * norm(test_embeddings[j]))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "labels_short = ['Q: Aspirin?', 'A: Aspirin cardio', 'A: Diabetes', 'Non-medical']\n",
    "sns.heatmap(sim_matrix, annot=True, fmt='.3f', xticklabels=labels_short,\n",
    "            yticklabels=labels_short, cmap='YlOrRd', ax=ax)\n",
    "ax.set_title('PubMedBERT Semantic Similarity Matrix')\n",
    "plt.tight_layout()\n",
    "plt.savefig('fig2_embedding_similarity.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('\\nKey observations:')\n",
    "print(f'  Aspirin Q vs Aspirin A: {sim_matrix[0][1]:.3f} (should be HIGH)')\n",
    "print(f'  Aspirin Q vs Diabetes:  {sim_matrix[0][2]:.3f} (should be medium)')\n",
    "print(f'  Aspirin Q vs Non-med:   {sim_matrix[0][3]:.3f} (should be LOW)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Vector Database & Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vector_store import MedicalVectorStore\n",
    "\n",
    "# Index a sample of the knowledge base (use more for production)\n",
    "SAMPLE_SIZE = 5000  # Increase to 50000+ for better retrieval\n",
    "sample_df = artificial_df.sample(n=SAMPLE_SIZE, random_state=42)\n",
    "print(f'Indexing {SAMPLE_SIZE} documents into ChromaDB...')\n",
    "\n",
    "store = MedicalVectorStore(config, embedder)\n",
    "store.create_index(sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test retrieval\n",
    "collection = store.get_collection()\n",
    "print(f'Total indexed chunks: {collection.count()}')\n",
    "\n",
    "# Sample query\n",
    "query = 'Does aspirin reduce the risk of heart attack?'\n",
    "results = store.query(query, top_k=5)\n",
    "\n",
    "print(f'\\nQuery: {query}')\n",
    "print(f'Retrieved {len(results[\"documents\"])} results:\\n')\n",
    "for i, (doc, dist, meta) in enumerate(zip(results['documents'], results['distances'], results['metadatas'])):\n",
    "    sim = 1 - (dist / 2)\n",
    "    print(f'  [{i+1}] Similarity: {sim:.4f} | PubMed: {meta[\"pubid\"]}')\n",
    "    print(f'      {doc[:150]}...\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Semantic Retriever with MMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from retriever import MedicalRetriever\n",
    "\n",
    "retriever = MedicalRetriever(config, store)\n",
    "\n",
    "test_queries = [\n",
    "    'Does aspirin reduce the risk of heart attack?',\n",
    "    'Is metformin effective for type 2 diabetes?',\n",
    "    'Can statins prevent stroke in elderly patients?',\n",
    "    'Does exercise help with depression?',\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    evidence = retriever.retrieve(query)\n",
    "    print(f'\\nQuery: {query}')\n",
    "    print(f'  Retrieved: {len(evidence)} passages')\n",
    "    if evidence:\n",
    "        print(f'  Top score: {evidence[0].score:.4f}')\n",
    "        print(f'  Top text: {evidence[0].text[:100]}...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize retrieval scores\n",
    "all_scores = []\n",
    "all_queries = []\n",
    "\n",
    "for query in test_queries:\n",
    "    evidence = retriever.retrieve(query, top_k=5)\n",
    "    for ev in evidence:\n",
    "        all_scores.append(ev.score)\n",
    "        all_queries.append(query[:30] + '...')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "score_df = pd.DataFrame({'Query': all_queries, 'Score': all_scores})\n",
    "sns.boxplot(data=score_df, x='Query', y='Score', ax=ax, palette='Set2')\n",
    "ax.set_title('Retrieval Similarity Scores by Query')\n",
    "ax.set_ylabel('Cosine Similarity')\n",
    "ax.tick_params(axis='x', rotation=30)\n",
    "plt.tight_layout()\n",
    "plt.savefig('fig3_retrieval_scores.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. LoRA Fine-tuning Configuration\n",
    "\n",
    "**Note:** Full fine-tuning takes several hours. Below shows the configuration and approach.\n",
    "To actually train, run: `python src/lora_finetune.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show LoRA configuration\n",
    "print('=== LoRA Configuration ===')\n",
    "print(f'Base Model:     {config[\"llm\"][\"base_model\"]}')\n",
    "print(f'LoRA Rank (r):  {config[\"lora\"][\"r\"]}')\n",
    "print(f'LoRA Alpha:     {config[\"lora\"][\"alpha\"]}')\n",
    "print(f'LoRA Dropout:   {config[\"lora\"][\"dropout\"]}')\n",
    "print(f'Target Modules: {config[\"lora\"][\"target_modules\"]}')\n",
    "print(f'\\n=== Training Configuration ===')\n",
    "print(f'Epochs:         {config[\"training\"][\"num_epochs\"]}')\n",
    "print(f'Batch Size:     {config[\"training\"][\"batch_size\"]}')\n",
    "print(f'Learning Rate:  {config[\"training\"][\"learning_rate\"]}')\n",
    "print(f'Max Seq Length: {config[\"training\"][\"max_seq_length\"]}')\n",
    "print(f'Output Dir:     {config[\"training\"][\"output_dir\"]}')\n",
    "\n",
    "# Calculate trainable parameters (estimated)\n",
    "r = config['lora']['r']\n",
    "# BioMistral-7B has ~7B params, LoRA adds r*d*2*num_modules params\n",
    "d = 4096  # hidden dim for 7B model\n",
    "num_modules = len(config['lora']['target_modules'])\n",
    "lora_params = r * d * 2 * num_modules * 32  # 32 layers\n",
    "total_params = 7_000_000_000\n",
    "\n",
    "print(f'\\n=== Parameter Efficiency ===')\n",
    "print(f'Total model params:    ~{total_params/1e9:.1f}B')\n",
    "print(f'LoRA trainable params: ~{lora_params/1e6:.1f}M')\n",
    "print(f'Trainable percentage:  {lora_params/total_params*100:.3f}%')\n",
    "print(f'Memory saved:          ~{(1 - lora_params/total_params)*100:.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize LoRA architecture\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Parameter comparison\n",
    "labels = ['Full Model\\n(7B params)', 'LoRA Adapters\\n(~17M params)']\n",
    "sizes = [total_params/1e9, lora_params/1e6/1000]\n",
    "axes[0].bar(labels, sizes, color=['#e74c3c', '#2ecc71'])\n",
    "axes[0].set_ylabel('Parameters (Billions)')\n",
    "axes[0].set_title('Full Fine-tuning vs LoRA Parameter Count')\n",
    "axes[0].set_yscale('log')\n",
    "\n",
    "# Training data composition\n",
    "train_labels = ['pqa_artificial\\n(Training)', 'pqa_labeled\\n(Evaluation)']\n",
    "train_sizes = [len(artificial_df), len(labeled_df)]\n",
    "axes[1].bar(train_labels, train_sizes, color=['#3498db', '#f39c12'])\n",
    "axes[1].set_ylabel('Number of Examples')\n",
    "axes[1].set_title('Training Data Composition')\n",
    "\n",
    "for i, v in enumerate(train_sizes):\n",
    "    axes[1].text(i, v + 2000, f'{v:,}', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('fig4_lora_overview.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. RAG Pipeline & Safety Guardrails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline import MedicalRAGPipeline\n",
    "from guardrails import ClinicalGuardrails\n",
    "\n",
    "# Initialize pipeline (without LLM for demo - uses rule-based fallback)\n",
    "pipeline = MedicalRAGPipeline(config)\n",
    "\n",
    "# Test medical query detection\n",
    "guardrails = ClinicalGuardrails(config)\n",
    "test_queries_safety = [\n",
    "    'Does aspirin reduce the risk of heart attack?',\n",
    "    'Is metformin effective for diabetes?',\n",
    "    'What is the weather today?',\n",
    "    'Can exercise help with depression?',\n",
    "    'What is the best pizza restaurant?',\n",
    "]\n",
    "\n",
    "print('=== Medical Query Detection ===')\n",
    "for q in test_queries_safety:\n",
    "    is_med = guardrails._is_medical_query(q)\n",
    "    print(f\"  {'[MEDICAL]  ' if is_med else '[NON-MED]  '} {q}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run full pipeline on sample questions\n",
    "medical_questions = [\n",
    "    'Does aspirin reduce the risk of heart attack?',\n",
    "    'Is metformin effective for type 2 diabetes?',\n",
    "    'Can statins prevent stroke?',\n",
    "    'Does exercise help with depression?',\n",
    "]\n",
    "\n",
    "responses = []\n",
    "for q in medical_questions:\n",
    "    print(f'\\n{\"=\"*70}')\n",
    "    response = pipeline.answer(q)\n",
    "    responses.append(response)\n",
    "    print(f'\\nQuestion: {response.question}')\n",
    "    print(f'Decision: {response.decision}')\n",
    "    print(f'Confidence: {response.confidence:.4f}')\n",
    "    print(f'Evidence count: {len(response.evidence_texts)}')\n",
    "    print(f'Safe: {response.is_safe}')\n",
    "    print(f'Warnings: {response.warnings}')\n",
    "    print(f'\\nTop evidence: {response.evidence_texts[0][:200]}...' if response.evidence_texts else 'No evidence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show full safe response for one example\n",
    "print('=== FULL SAFE RESPONSE ===')\n",
    "print(responses[0].safe_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Evaluation on PubMedQA Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import PipelineEvaluator\n",
    "\n",
    "evaluator = PipelineEvaluator(pipeline, config)\n",
    "\n",
    "# Evaluate retrieval on test set\n",
    "retrieval_metrics = evaluator.evaluate_retrieval(test_df.head(50))  # Use subset for speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate generation (using rule-based fallback since LLM not loaded in notebook)\n",
    "gen_metrics = evaluator.evaluate_generation(test_df.head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "details = gen_metrics['details']\n",
    "evaluator.plot_confusion_matrix(\n",
    "    details['true'].tolist(), details['predicted'].tolist(),\n",
    "    save_path='fig5_confusion_matrix.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confidence distribution\n",
    "evaluator.plot_confidence_distribution(details, save_path='fig6_confidence_dist.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot safety analysis\n",
    "evaluator.plot_safety_analysis(details, save_path='fig7_safety_analysis.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive metrics summary\n",
    "print('\\n' + '='*60)\n",
    "print('EVALUATION SUMMARY')\n",
    "print('='*60)\n",
    "\n",
    "print(f'\\n--- Retrieval Metrics ---')\n",
    "print(f'  Hit Rate:         {retrieval_metrics[\"hit_rate\"]:.4f}')\n",
    "print(f'  MRR:              {retrieval_metrics[\"mrr\"]:.4f}')\n",
    "print(f'  Avg Similarity:   {retrieval_metrics[\"avg_similarity\"]:.4f}')\n",
    "\n",
    "print(f'\\n--- Generation Metrics ---')\n",
    "print(f'  Accuracy:         {gen_metrics[\"accuracy\"]:.4f}')\n",
    "print(f'  F1 (macro):       {gen_metrics[\"f1_macro\"]:.4f}')\n",
    "print(f'  F1 (weighted):    {gen_metrics[\"f1_weighted\"]:.4f}')\n",
    "\n",
    "print(f'\\n--- Per-Class Performance ---')\n",
    "for label, metrics in gen_metrics['per_class'].items():\n",
    "    print(f'  {label:6s}: P={metrics[\"precision\"]:.3f} R={metrics[\"recall\"]:.3f} F1={metrics[\"f1\"]:.3f} (n={metrics[\"support\"]})')\n",
    "\n",
    "print(f'\\n--- Safety Metrics ---')\n",
    "safe_rate = details['is_safe'].mean() * 100\n",
    "print(f'  Safety Pass Rate: {safe_rate:.1f}%')\n",
    "print(f'  Avg Warnings:     {details[\"num_warnings\"].mean():.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Pipeline Architecture Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create architecture diagram\n",
    "fig, ax = plt.subplots(figsize=(16, 10))\n",
    "ax.set_xlim(0, 10)\n",
    "ax.set_ylim(0, 10)\n",
    "ax.axis('off')\n",
    "\n",
    "# Title\n",
    "ax.text(5, 9.5, 'Medical RAG Pipeline Architecture', ha='center', fontsize=18, fontweight='bold')\n",
    "\n",
    "# Row 1: Indexing Pipeline\n",
    "ax.text(0.5, 8.5, 'INDEXING PIPELINE', fontsize=12, fontweight='bold', color='#2c3e50')\n",
    "boxes_row1 = [\n",
    "    (1, 7.5, 'PubMedQA\\n(211K docs)', '#3498db'),\n",
    "    (3, 7.5, 'Chunker\\n(256 tokens)', '#2ecc71'),\n",
    "    (5, 7.5, 'PubMedBERT\\nEmbeddings', '#e74c3c'),\n",
    "    (7, 7.5, 'ChromaDB\\nVector Store', '#9b59b6'),\n",
    "]\n",
    "for x, y, text, color in boxes_row1:\n",
    "    ax.add_patch(plt.Rectangle((x-0.7, y-0.4), 1.4, 0.8, fill=True, \n",
    "                                facecolor=color, alpha=0.3, edgecolor=color, linewidth=2))\n",
    "    ax.text(x, y, text, ha='center', va='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "for x in [2, 4, 6]:\n",
    "    ax.annotate('', xy=(x+0.3, 7.5), xytext=(x-0.3, 7.5),\n",
    "                arrowprops=dict(arrowstyle='->', color='black', lw=2))\n",
    "\n",
    "# Row 2: Query Pipeline  \n",
    "ax.text(0.5, 6, 'QUERY PIPELINE', fontsize=12, fontweight='bold', color='#2c3e50')\n",
    "boxes_row2 = [\n",
    "    (1, 5, 'User\\nQuery', '#f39c12'),\n",
    "    (3, 5, 'PubMedBERT\\nEmbed Query', '#e74c3c'),\n",
    "    (5, 5, 'Retriever\\n(MMR, Top-5)', '#2ecc71'),\n",
    "    (7, 5, 'Evidence\\nPassages', '#3498db'),\n",
    "]\n",
    "for x, y, text, color in boxes_row2:\n",
    "    ax.add_patch(plt.Rectangle((x-0.7, y-0.4), 1.4, 0.8, fill=True,\n",
    "                                facecolor=color, alpha=0.3, edgecolor=color, linewidth=2))\n",
    "    ax.text(x, y, text, ha='center', va='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "for x in [2, 4, 6]:\n",
    "    ax.annotate('', xy=(x+0.3, 5), xytext=(x-0.3, 5),\n",
    "                arrowprops=dict(arrowstyle='->', color='black', lw=2))\n",
    "\n",
    "# Row 3: Generation Pipeline\n",
    "ax.text(0.5, 3.5, 'GENERATION PIPELINE', fontsize=12, fontweight='bold', color='#2c3e50')\n",
    "boxes_row3 = [\n",
    "    (2, 2.5, 'Prompt\\nComposer', '#f39c12'),\n",
    "    (4.5, 2.5, 'BioMistral-7B\\n+ LoRA', '#e74c3c'),\n",
    "    (7, 2.5, 'Safety\\nGuardrails', '#27ae60'),\n",
    "    (9, 2.5, 'Final\\nResponse', '#9b59b6'),\n",
    "]\n",
    "for x, y, text, color in boxes_row3:\n",
    "    ax.add_patch(plt.Rectangle((x-0.7, y-0.4), 1.4, 0.8, fill=True,\n",
    "                                facecolor=color, alpha=0.3, edgecolor=color, linewidth=2))\n",
    "    ax.text(x, y, text, ha='center', va='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "for x1, x2 in [(2.7, 3.8), (5.2, 6.3), (7.7, 8.3)]:\n",
    "    ax.annotate('', xy=(x2, 2.5), xytext=(x1, 2.5),\n",
    "                arrowprops=dict(arrowstyle='->', color='black', lw=2))\n",
    "\n",
    "# Connect rows\n",
    "ax.annotate('', xy=(7, 5.4), xytext=(7, 7.1),\n",
    "            arrowprops=dict(arrowstyle='->', color='gray', lw=1.5, ls='--'))\n",
    "ax.annotate('', xy=(2, 2.9), xytext=(7, 4.6),\n",
    "            arrowprops=dict(arrowstyle='->', color='gray', lw=1.5, ls='--'))\n",
    "\n",
    "# Legend\n",
    "ax.text(1, 1, 'Key: Blue=Data | Green=Processing | Red=Models | Purple=Output | Yellow=Input',\n",
    "        fontsize=9, style='italic', color='gray')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('fig8_architecture.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "### What Was Built\n",
    "A complete Retrieval-Augmented Generation (RAG) pipeline for medical question answering with:\n",
    "\n",
    "1. **Data Pipeline**: Loaded and preprocessed 212K PubMedQA examples with train/val/test splits\n",
    "2. **PubMedBERT Embeddings**: Domain-specific biomedical embeddings for semantic retrieval\n",
    "3. **ChromaDB Vector Store**: Indexed knowledge base with chunked abstracts\n",
    "4. **MMR Retriever**: Semantic search with diversity-preserving reranking\n",
    "5. **BioMistral-7B + QLoRA**: Parameter-efficient fine-tuning (0.24% trainable params)\n",
    "6. **Safety Guardrails**: Disclaimers, hallucination detection, scope filtering, confidence checks\n",
    "7. **Evaluation Suite**: Retrieval metrics (MRR, Hit Rate), Generation metrics (Accuracy, F1), Safety analysis\n",
    "\n",
    "### Key Design Decisions\n",
    "- **PubMedBERT > general BERT**: 10-15% better at biomedical semantic similarity\n",
    "- **QLoRA (4-bit)**: Reduces memory from 28GB to ~6GB, enabling training on consumer hardware\n",
    "- **MMR reranking**: Prevents redundant evidence, improves answer diversity\n",
    "- **Safety-first**: Every response includes disclaimers, confidence scores, and evidence sources\n",
    "\n",
    "### How to Use in Production\n",
    "1. Run `python src/lora_finetune.py` for full model training\n",
    "2. Index the full 211K knowledge base: increase `SAMPLE_SIZE` in vector_store\n",
    "3. Load fine-tuned model: `from lora_finetune import load_finetuned_model`\n",
    "4. Run pipeline: `pipeline.answer('your medical question')`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
